{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bb654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from barbar import Bar\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "from self_supervised.tasks import fast_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(use_gpu)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'fed-chex_res18'\n",
    "        self.dataset_name = 'Chexpert'\n",
    "        self.save_path = './checkpoint/' + self.name\n",
    "        self.model_path = self.save_path + '/models'\n",
    "\n",
    "        self.num_threads = 8\n",
    "        self.shuffle_dataset=True\n",
    "        self.random_seed=24\n",
    "        self.shuffle = False\n",
    "\n",
    "        self.lr = 0.002      \n",
    "        \n",
    "        self.serial_batches = False\n",
    "        self.phase='train'\n",
    "        \n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 1\n",
    "        self.valid_size = 0.04\n",
    "        self.test_size  = 0.5\n",
    "        self.train_size = 0.1\n",
    "        self.num_classes = 13\n",
    "        \n",
    "        self.max_epochs = 200\n",
    " \n",
    "\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "        os.makedirs(self.model_path, exist_ok=True)\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd953dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Traindata = pd.read_csv('/workspace/DATASETS/CheXpert-v1.0-small/train.csv')\n",
    "\n",
    "Traindata = Traindata[Traindata['Path'].str.contains(\"frontal\")] # use only frontal images\n",
    "# Traindata = Traindata[500:]\n",
    "# Traindata.to_csv('/workspace/DATASETS/CheXpert-v1.0-small/train_mod.csv', index = False)\n",
    "# print(\"Train data length:\", len(Traindata))\n",
    "\n",
    "Validdata = pd.read_csv('/workspace/DATASETS/CheXpert-v1.0-small/valid.csv')\n",
    "Validdata = Validdata[Validdata['Path'].str.contains(\"frontal\")] # use only frontal images\n",
    "# Validdata.to_csv('/workspace/DATASETS/CheXpert-v1.0-small/valid_mod.csv', index = False)\n",
    "# print(\"Valid data length:\", len(Validdata))\n",
    "\n",
    "Testdata = Traindata.head(25000) # use first 500 training data as test data (obs ratio is almost same!)\n",
    "# Testdata.to_csv('/workspace/DATASETS/CheXpert-v1.0-small/test_mod.csv', index = False)\n",
    "# print(\"Test data length:\", len(Testdata))\n",
    "\n",
    "pathFileTrain = '/workspace/DATASETS/CheXpert-v1.0-small/train_mod.csv'\n",
    "pathFileValid = '/workspace/DATASETS/CheXpert-v1.0-small/valid_mod.csv'\n",
    "pathFileTest = '/workspace/DATASETS/CheXpert-v1.0-small/test_mod.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv = pd.read_csv('/workspace/DATASETS/CheXpert-v1.0-small/train.csv')\n",
    "# csv = csv[csv['Path'].str.contains(\"frontal\")]   # use only frontal images\n",
    "# csv = csv.sample(frac = 1)\n",
    "# print(\"Total data length:\", len(csv))\n",
    "# Traindata = csv[:170000]\n",
    "# print(\"Train data length:\", len(Traindata))\n",
    "# Testdata = csv[170000:]\n",
    "# print(\"Test data length:\", len(Testdata))\n",
    "# Validdata = pd.read_csv('/workspace/DATASETS/CheXpert-v1.0-small/valid_mod.csv')\n",
    "# Validdata = Validdata[Validdata['Path'].str.contains(\"frontal\")]\n",
    "# print(\"Valid data length:\", len(Validdata))\n",
    "\n",
    "# # Traindata.to_csv('/workspace/DATASETS/CheXpert-v1.0-small/train_mod.csv', index = False)\n",
    "# # Validdata.to_csv('/workspace/DATASETS/CheXpert-v1.0-small/valid_mod.csv', index = False)\n",
    "# # Testdata.to_csv('/workspace/DATASETS/CheXpert-v1.0-small/test_mod.csv', index = False)\n",
    "\n",
    "# pathFileTrain = '/workspace/DATASETS/CheXpert-v1.0-small/train_mod.csv'\n",
    "# pathFileValid = '/workspace/DATASETS/CheXpert-v1.0-small/valid_mod.csv'\n",
    "# pathFileTest = '/workspace/DATASETS/CheXpert-v1.0-small/test_mod.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network parameters:\n",
    "nnIsTrained = False     # pre-trained using ImageNet\n",
    "\n",
    "# Training settings: batch size, maximum number of epochs\n",
    "trBatchSize = 16\n",
    "trMaxEpoch = 3\n",
    "\n",
    "\n",
    "nnIsTrained = False     # pre-trained using ImageNet\n",
    "nnClassCount = 14       # dimension of the output\n",
    "\n",
    "imgtransResize = (320, 320)\n",
    "imgtransCrop = 224\n",
    "trMaxEpoch = 500\n",
    "\n",
    "# Class names\n",
    "class_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n",
    "               'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "               'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ceba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDataSet(Dataset):\n",
    "    def __init__(self, data_PATH, transform = None, policy = \"ones\"):\n",
    "        \"\"\"\n",
    "        data_PATH: path to the file containing images with corresponding labels.\n",
    "        transform: optional transform to be applied on a sample.\n",
    "        Upolicy: name the policy with regard to the uncertain labels.\n",
    "        \"\"\"\n",
    "        image_names = []\n",
    "        labels = []\n",
    "\n",
    "        with open(data_PATH, \"r\") as f:\n",
    "            csvReader = csv.reader(f)\n",
    "            next(csvReader, None) # skip the header\n",
    "            for line in csvReader:\n",
    "                image_name = line[0]\n",
    "                label = line[5:]\n",
    "                \n",
    "                for i in range(14):\n",
    "                    if label[i]:\n",
    "                        a = float(label[i])\n",
    "                        if a == 1:\n",
    "                            label[i] = 1\n",
    "                        elif a == -1:\n",
    "                            if policy == \"ones\":\n",
    "                                label[i] = 1\n",
    "                            elif policy == \"zeroes\":\n",
    "                                label[i] = 0\n",
    "                            else:\n",
    "                                label[i] = 0\n",
    "                        else:\n",
    "                            label[i] = 0\n",
    "                    else:\n",
    "                        label[i] = 0\n",
    "                        \n",
    "                image_names.append(image_name)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image and its labels\"\"\"\n",
    "        image_name = self.image_names[index]\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]  # mean of ImageNet dataset(for normalization)\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]   # std of ImageNet dataset(for normalization)\n",
    "# Tranform data\n",
    "normalize = transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "transformList = []\n",
    "transformList.append(transforms.Resize((imgtransCrop, imgtransCrop)))\n",
    "transformList.append(transforms.RandomResizedCrop(imgtransCrop))\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)\n",
    "transformSequence = transforms.Compose(transformList)\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "datasetTrain = CheXpertDataSet(pathFileTrain, transformSequence, policy = \"ones\")\n",
    "print(\"Train data length:\", len(datasetTrain))\n",
    "\n",
    "datasetValid = CheXpertDataSet(pathFileValid, transformSequence)\n",
    "print(\"Valid data length:\", len(datasetValid))\n",
    "\n",
    "datasetTest = CheXpertDataSet(pathFileTest, transformSequence, policy = \"ones\")\n",
    "print(\"Test data length:\", len(datasetTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be03729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(datasetTrain,batch_size=opt.batch_size,shuffle=True,num_workers=8,pin_memory=True)\n",
    "# val_loader = DataLoader(datasetValid,batch_size=opt.batch_size,shuffle=True,num_workers=8,pin_memory=True)\n",
    "# test_loader = DataLoader(datasetTest,batch_size=opt.batch_size,shuffle=True,num_workers=8,pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "# FOR MULTIPLE COMMUNICATION ROUNDS\n",
    "com_round = 3\n",
    "fraction = 1.0\n",
    "epoch = 3\n",
    "batch = 48\n",
    "num_clients = 5\n",
    "\n",
    "'''\n",
    "# Divide datasetTrain_ex\n",
    "datasetTrain_1, datasetTrain_2, datasetTrain_3, datasetTrain_4, datasetTrain_5, dataleft = random_split(datasetTrain, \n",
    "                                                                                                        [100, 100, 100, 100, 100,\n",
    "                                                                                                         len(datasetTrain) - 500])\n",
    "'''\n",
    "# Divide datasetTrain_real\n",
    "datasetTrain_1, datasetTrain_2, datasetTrain_3, datasetTrain_4, datasetTrain_5 = random_split(datasetTrain, \n",
    "                                                                                              [34000,34000,34000,34000,34000])\n",
    "\n",
    "\n",
    "# Define 5 DataLoaders\n",
    "dataLoaderTrain_1 = DataLoader(dataset = datasetTrain_1, batch_size = trBatchSize,\n",
    "                               shuffle = True, num_workers = 8, pin_memory = True)\n",
    "dataLoaderTrain_2 = DataLoader(dataset = datasetTrain_2, batch_size = trBatchSize,\n",
    "                               shuffle = True, num_workers = 8, pin_memory = True)\n",
    "dataLoaderTrain_3 = DataLoader(dataset = datasetTrain_3, batch_size = trBatchSize,\n",
    "                               shuffle = True, num_workers = 8, pin_memory = True)\n",
    "dataLoaderTrain_4 = DataLoader(dataset = datasetTrain_4, batch_size = trBatchSize,\n",
    "                               shuffle = True, num_workers = 8, pin_memory = True)\n",
    "dataLoaderTrain_5 = DataLoader(dataset = datasetTrain_5, batch_size = trBatchSize,\n",
    "                               shuffle = True, num_workers = 8, pin_memory = True)\n",
    "\n",
    "# Define Valid and Test DataLoaders\n",
    "dataLoaderVal = DataLoader(dataset = datasetValid, batch_size = trBatchSize, \n",
    "                           shuffle = False, num_workers = 8, pin_memory = True)\n",
    "dataLoaderTest = DataLoader(dataset = datasetTest, num_workers = 8, pin_memory = True)\n",
    "dT = [datasetTrain_1, datasetTrain_2, datasetTrain_3, datasetTrain_4, datasetTrain_5]\n",
    "dLT = [dataLoaderTrain_1, dataLoaderTrain_2, dataLoaderTrain_3, dataLoaderTrain_4, dataLoaderTrain_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard ResNet50\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet18 = torchvision.models.resnet18(pretrained = False)\n",
    "        num_ftrs = self.resnet18.fc.in_features\n",
    "        self.resnet18.fc = nn.Sequential(nn.Linear(num_ftrs, out_size),\n",
    "                                        nn.Sigmoid()\n",
    "                                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet18(x)\n",
    "        return x\n",
    "    \n",
    "model = ResNet18(nnClassCount).to(device)\n",
    "checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61576af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAUROC(dataGT, dataPRED, classCount):\n",
    "    # Computes area under ROC curve \n",
    "    # dataGT: ground truth data\n",
    "    # dataPRED: predicted data\n",
    "    outAUROC = []\n",
    "    datanpGT = dataGT.cpu().numpy()\n",
    "    datanpPRED = dataPRED.cpu().numpy()\n",
    "\n",
    "    for i in range(classCount):\n",
    "        try:\n",
    "            outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return outAUROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochTrain(model, dataLoaderTrain, optimizer, epochMax, classCount, loss):\n",
    "    losstrain = 0\n",
    "    model.train()\n",
    "\n",
    "    for batchID, (varInput, target) in enumerate(Bar(dataLoaderTrain)):\n",
    "        varTarget = target.cuda(non_blocking = True)\n",
    "        varInput = varInput.cuda(non_blocking = True)\n",
    "        varOutput = model(varInput)\n",
    "        lossvalue = loss(varOutput, varTarget)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lossvalue.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losstrain += lossvalue.item()\n",
    "\n",
    "    return losstrain / len(dataLoaderTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochVal(model, dataLoaderVal, optimizer, epochMax, ClassCount, loss):\n",
    "    model.eval()\n",
    "    lossVal = 0\n",
    "    outGT = torch.FloatTensor().cuda()\n",
    "    outPRED = torch.FloatTensor().cuda()\n",
    "    print('classCount :',ClassCount)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (varInput, target) in enumerate(dataLoaderVal):\n",
    "            \n",
    "            target = target.cuda(non_blocking = True)\n",
    "            outGT = torch.cat((outGT, target),0)\n",
    "            outGT = outGT.cuda(non_blocking = True)\n",
    "            varInput = varInput.cuda(non_blocking = True)\n",
    "            varOutput = model(varInput)\n",
    "            outPRED = torch.cat((outPRED,varOutput), 0)\n",
    "            lossVal += loss(varOutput, target)\n",
    "        aurocIndividual = computeAUROC(outGT, outPRED, ClassCount)\n",
    "        aurocMean = np.array(aurocIndividual).mean()\n",
    "#         print('AUROC mean ', aurocMean)\n",
    "\n",
    "    return lossVal / len(dataLoaderVal),aurocIndividual,aurocMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataLoaderTrain, dataLoaderVal, nnClassCount, trMaxEpoch, checkpoint):\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0) \n",
    "    loss = torch.nn.BCELoss() \n",
    "\n",
    "    if checkpoint != None and use_gpu: # loading checkpoint\n",
    "        modelCheckpoint = torch.load(checkpoint)\n",
    "        model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "        optimizer.load_state_dict(modelCheckpoint['optimizer'])\n",
    "\n",
    "    # Train the network\n",
    "    lossMIN = 100000\n",
    "    train_start = []\n",
    "    train_end = []\n",
    "    for epochID in range(0, trMaxEpoch):\n",
    "        train_start.append(time.time()) # training starts\n",
    "        losst = epochTrain(model, dataLoaderTrain, optimizer, trMaxEpoch, nnClassCount, loss)\n",
    "        train_end.append(time.time()) # training ends\n",
    "        lossv,aurocIndividual,aurocMean = epochVal(model, dataLoaderVal, optimizer, trMaxEpoch, nnClassCount, loss)\n",
    "        \n",
    "        print(\"Training loss: {:.3f},\".format(losst), \"Valid loss: {:.3f}\".format(lossv),\"Valid auc: {:.3f}\".format(aurocMean))\n",
    "        if lossv < lossMIN:\n",
    "            lossMIN = lossv\n",
    "            torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), \n",
    "                        'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, \n",
    "                        os.path.join(opt.model_path,'resnet50_m-epoch' + str(epochID + 1) + '.pth.tar'))\n",
    "            \n",
    "            print('Epoch ' + str(epochID + 1) + ' [save] loss = ' + str(lossv.item()))\n",
    "        else:\n",
    "            print('Epoch ' + str(epochID + 1) + ' [----] loss = ' + str(lossv.item()))\n",
    "\n",
    "    train_time = np.array(train_end) - np.array(train_start)\n",
    "    print(\"Training time for each epoch: {} seconds\".format(train_time.round(0)))\n",
    "    params = model.state_dict()\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfcbe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataLoaderTest, nnClassCount, checkpoint, class_names):\n",
    "    cudnn.benchmark = True\n",
    "    use_gpu = True\n",
    "    if checkpoint != None:\n",
    "        modelCheckpoint = torch.load(checkpoint,map_location=device)\n",
    "        model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "#         model = model.to(device)\n",
    "    if use_gpu:\n",
    "        outGT = torch.FloatTensor().cuda()\n",
    "        outPRED = torch.FloatTensor().cuda()\n",
    "    else:\n",
    "        outGT = torch.FloatTensor()\n",
    "        outPRED = torch.FloatTensor()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(dataLoaderTest):\n",
    "            target = target.cuda()\n",
    "            outGT = torch.cat((outGT, target), 0).cuda()\n",
    "            bs, c, h, w = input.size()\n",
    "            varInput = input.view(-1, c, h, w).cuda()\n",
    "\n",
    "            out = model(varInput)\n",
    "            outPRED = torch.cat((outPRED, out), 0)\n",
    "    aurocIndividual = computeAUROC(outGT, outPRED, nnClassCount)\n",
    "    aurocMean = np.array(aurocIndividual).mean()\n",
    "    print('AUROC mean ', aurocMean)\n",
    "\n",
    "    for i in range (0, len(aurocIndividual)):\n",
    "        print(class_names[i], ' ', aurocIndividual[i])\n",
    "\n",
    "    return outGT, outPRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(com_round):\n",
    "    print(\"[[[ Round {} Start ]]]\".format(i + 1))\n",
    "    params = [None] * num_clients\n",
    "    sel_clients = sorted(random.sample(range(num_clients),round(num_clients*fraction))) # Step 1: select random fraction of clients\n",
    "    print(\"The number of clients:\", len(sel_clients))\n",
    "    \n",
    "    for j in sel_clients: # Step 2: send weights to clients\n",
    "        print(\"<< Client {} Training Start >>\".format(j + 1))\n",
    "        train_valid_start = time.time()\n",
    "        params[j] = train(model, dLT[j], dataLoaderTest, # Step 3: Perform local computations\n",
    "                                          nnClassCount, trMaxEpoch = epoch, checkpoint = None)\n",
    "        train_valid_end = time.time()\n",
    "        client_time = round(train_valid_end - train_valid_start)\n",
    "        print(\"<< Client {} Training End: {} seconds elapsed >>\".format(j + 1, client_time))\n",
    "        \n",
    "    fidx = [idx for idx in range(len(params)) if params[idx] != None][0]\n",
    "    lidx = [idx for idx in range(len(params)) if params[idx] != None][-1]\n",
    "    \n",
    "    for key in params[fidx]: # Step 4: return updates to server\n",
    "        weights, weightn = [], []\n",
    "        for k in sel_clients:\n",
    "            weights.append(params[k][key]*len(dT[k]))\n",
    "            weightn.append(len(dT[k]))\n",
    "        params[lidx][key] = sum(weights) / sum(weightn) # weighted averaging model weights\n",
    "\n",
    "    model = ResNet18(nnClassCount).to(device)\n",
    "    model.load_state_dict(params[lidx]) # Step 5: server updates global state\n",
    "    print(\"[[[ Round {} End ]]]\".format(i + 1))\n",
    "    \n",
    "print(\"Global model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e61fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
